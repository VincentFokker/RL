{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint for rank 0\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.0090\n",
      "Function value obtained: 503.4000\n",
      "Current minimum: 503.4000\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t9\n",
      "wrong_sup_at_goal\t\t:\t5\n",
      "    flooding_reward\t\t\t:\t9\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t7\n",
      "    negative_reward_for_cycle\t:\t5\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\_vinc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\skopt\\space\\space.py:502: RuntimeWarning: divide by zero encountered in log10\n",
      "  np.log10(self.low) / self.log_base,\n",
      "c:\\users\\_vinc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\skopt\\space\\space.py:504: RuntimeWarning: divide by zero encountered in log10\n",
      "  np.log10(self.low) / self.log_base)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 127.2893\n",
      "Function value obtained: -507.9000\n",
      "Current minimum: -507.9000\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t7\n",
      "wrong_sup_at_goal\t\t:\t6\n",
      "    flooding_reward\t\t\t:\t5\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t4\n",
      "    negative_reward_for_cycle\t:\t8\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2106\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 136.5459\n",
      "Function value obtained: -419.4000\n",
      "Current minimum: -507.9000\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t4\n",
      "    flooding_reward\t\t\t:\t10\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t4\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2108\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 130.4359\n",
      "Function value obtained: -725.6000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t4\n",
      "wrong_sup_at_goal\t\t:\t7\n",
      "    flooding_reward\t\t\t:\t6\n",
      "neg_reward_ia\t\t\t:\t4\n",
      "negative_reward_for_empty_queue\t:\t10\n",
      "    negative_reward_for_cycle\t:\t5\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2110\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 128.7909\n",
      "Function value obtained: -425.7000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t5\n",
      "wrong_sup_at_goal\t\t:\t8\n",
      "    flooding_reward\t\t\t:\t6\n",
      "neg_reward_ia\t\t\t:\t10\n",
      "negative_reward_for_empty_queue\t:\t7\n",
      "    negative_reward_for_cycle\t:\t9\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2112\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 138.6122\n",
      "Function value obtained: -509.0000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t8\n",
      "wrong_sup_at_goal\t\t:\t7\n",
      "    flooding_reward\t\t\t:\t4\n",
      "neg_reward_ia\t\t\t:\t10\n",
      "negative_reward_for_empty_queue\t:\t7\n",
      "    negative_reward_for_cycle\t:\t6\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2115\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 139.8486\n",
      "Function value obtained: -480.1000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t4\n",
      "wrong_sup_at_goal\t\t:\t5\n",
      "    flooding_reward\t\t\t:\t5\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t9\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2117\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 137.2700\n",
      "Function value obtained: -437.9000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t5\n",
      "wrong_sup_at_goal\t\t:\t6\n",
      "    flooding_reward\t\t\t:\t5\n",
      "neg_reward_ia\t\t\t:\t9\n",
      "negative_reward_for_empty_queue\t:\t7\n",
      "    negative_reward_for_cycle\t:\t5\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2119\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 132.4504\n",
      "Function value obtained: -460.4000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t6\n",
      "    flooding_reward\t\t\t:\t9\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t5\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2122\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 137.8856\n",
      "Function value obtained: -692.4000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t10\n",
      "    flooding_reward\t\t\t:\t9\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t6\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2124\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 131.2439\n",
      "Function value obtained: -554.9000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t4\n",
      "    flooding_reward\t\t\t:\t9\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t5\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2126\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 144.6144\n",
      "Function value obtained: -654.8000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t10\n",
      "    flooding_reward\t\t\t:\t4\n",
      "neg_reward_ia\t\t\t:\t8\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t5\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2128\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 148.0486\n",
      "Function value obtained: -710.2000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t5\n",
      "    flooding_reward\t\t\t:\t4\n",
      "neg_reward_ia\t\t\t:\t4\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t10\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2131\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 134.2425\n",
      "Function value obtained: -440.8000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t9\n",
      "    flooding_reward\t\t\t:\t8\n",
      "neg_reward_ia\t\t\t:\t10\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t4\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2133\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 132.6834\n",
      "Function value obtained: -506.9000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t5\n",
      "    flooding_reward\t\t\t:\t10\n",
      "neg_reward_ia\t\t\t:\t10\n",
      "negative_reward_for_empty_queue\t:\t4\n",
      "    negative_reward_for_cycle\t:\t8\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2135\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 143.2312\n",
      "Function value obtained: -532.7000\n",
      "Current minimum: -725.6000\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "model loaded from path: D:\\Drive\\git\\RL\\rl\\config\\AbstractConveyor.yml\n",
      "Current settings for the config: \n",
      "\n",
      "positive_reward_for_divert \t:\t10\n",
      "wrong_sup_at_goal\t\t:\t4\n",
      "    flooding_reward\t\t\t:\t9\n",
      "neg_reward_ia\t\t\t:\t10\n",
      "negative_reward_for_empty_queue\t:\t8\n",
      "    negative_reward_for_cycle\t:\t10\n",
      "\n",
      "Results stored in: D:\\Drive\\git\\RL\\rl\\trained_models\\AbstractConveyor\\20201123_2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:    14, steps: 557, R: -24.000\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from os.path import join, isfile\n",
    "from hyperspace import hyperdrive\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "import rl\n",
    "\n",
    "env_name = 'AbstractConveyor'\n",
    "\n",
    "\n",
    "path = pathlib.Path().absolute()\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function to be minimized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    * params [list, len(params)=n_hyperparameters]\n",
    "        Settings of each hyperparameter for a given optimization iteration.\n",
    "        - Controlled by hyperspaces's hyperdrive function.\n",
    "        - Order preserved from list passed to hyperdrive's hyperparameters argument.\n",
    "     \"\"\"\n",
    "    config_path = join(path, 'rl', 'config', '{}.yml'.format(env_name))\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        print('model loaded from path: {}'.format(config_path))\n",
    "    \n",
    "    #set the parameters\n",
    "    prfd, wsag, fr, nria, nrfeq, nrfc = params\n",
    "    config['environment']['positive_reward_for_divert'] = prfd\n",
    "    config['environment']['wrong_sup_at_goal'] = wsag\n",
    "    config['environment']['flooding_reward'] = fr\n",
    "    config['environment']['neg_reward_ia'] = nria\n",
    "    config['environment']['negative_reward_for_empty_queue'] = nrfeq\n",
    "    config['environment']['negative_reward_for_cycle'] = nrfc\n",
    "    \n",
    "    print('Current settings for the config: \\n\\npositive_reward_for_divert \\t:\\t{}\\nwrong_sup_at_goal\\t\\t:\\t{}\\n\\\n",
    "flooding_reward\\t\\t\\t:\\t{}\\nneg_reward_ia\\t\\t\\t:\\t{}\\nnegative_reward_for_empty_queue\\t:\\t{}\\n\\\n",
    "negative_reward_for_cycle\\t:\\t{}\\n'.format(prfd, wsag, fr, nria, nrfeq, nrfc))\n",
    "    \n",
    "    #GET MODEL CONFIG\n",
    "    model_config = config['models']['PPO2']\n",
    "    policy = config['main']['policy']\n",
    "    n_workers = config['main']['n_workers']\n",
    "    n_steps = config['main']['n_steps']\n",
    "    n_eval = (n_steps / 8)/10\n",
    "    \n",
    "    # load environment with config variables\n",
    "    env_obj = getattr(rl.environments, env_name)\n",
    "    env = env_obj(config)\n",
    "    \n",
    "    # multiprocess environment\n",
    "    env_8 = make_vec_env(lambda: env, n_envs=n_workers)\n",
    "    \n",
    "    #define folder and path\n",
    "    now = datetime.datetime.now()\n",
    "    folder ='{}{}{}_{}{}'.format(now.year, str(now.month).zfill(2), str(now.day).zfill(2), str(now.hour).zfill(2), str(now.minute).zfill(2))\n",
    "    specified_path = join(path, 'rl', 'trained_models', env_name, folder)\n",
    "    print('Results stored in: {}'.format(specified_path))\n",
    "    \n",
    "    # callback for evaluation\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=specified_path,\n",
    "                                 log_path=specified_path, eval_freq=n_eval,\n",
    "                                 n_eval_episodes=5, verbose=0,\n",
    "                                 deterministic=False, render=False)\n",
    "\n",
    "    model = PPO2(policy, env=env_8, tensorboard_log=specified_path, **model_config)\n",
    "    \n",
    "    #LEARN MODEL\n",
    "    model.learn(total_timesteps=n_steps, tb_log_name='{}_{}_{}_{}_{}_{}'.format(prfd, wsag, fr, nria, nrfeq, nrfc),\n",
    "                        callback=eval_callback)\n",
    "    model_path = join(specified_path, 'model_{}_{}_{}_{}_{}_{}.zip'.format(prfd, wsag, fr, nria, nrfeq, nrfc))\n",
    "    model.save(model_path)\n",
    "    \n",
    "    #test\n",
    "    best_modelpath = join(specified_path, 'best_model.zip')\n",
    "    test_model = PPO2.load(best_modelpath, env=DummyVecEnv([lambda: env]))\n",
    "    \n",
    "    #run test of the model\n",
    "    episodes = 10\n",
    "    results = {}\n",
    "    results['cycle_count'] = 0\n",
    "    results['idle_time'] = 0\n",
    "    for episode in range(episodes):\n",
    "        # Run an episode\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        meta_data = []\n",
    "        while not done:\n",
    "            action, _ = test_model.predict(state, deterministic=False)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                results['cycle_count'] += env.cycle_count\n",
    "                results['idle_time'] += sum(env.idle_times_operator.values())\n",
    "    \n",
    "    return -1*((results['cycle_count'] + results['idle_time']) /episodes)\n",
    "    \n",
    "def main():\n",
    "    #hparams = [(low, high),        #per var\n",
    "    #           (low, high)]\n",
    "    hparams = [(0, 10), #positive_reward_for_divert\n",
    "               (0, 10), #wrong_sup_at_goal\n",
    "               (0, 10), #flooding_reward\n",
    "               (0, 10), #neg_reward_ia\n",
    "               (0, 10), #negative_reward_for_empty_queue\n",
    "               (0, 10)] #negative_reward_for_cycle\n",
    "    \n",
    "    #define path for the results\n",
    "    hyperdive_results = join(path, 'rl', 'hyper_parameter', env_name)\n",
    "    \n",
    "    #make folder if not exist\n",
    "    try:\n",
    "        os.mkdir(hyperdive_results)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #run the hyper drive optimization\n",
    "    hyperdrive(objective=objective,\n",
    "               hyperparameters=hparams,\n",
    "               results_path=hyperdive_results,\n",
    "               checkpoints_path=hyperdive_results,\n",
    "               model=\"GP\",\n",
    "               n_iterations=50,\n",
    "               verbose=True,\n",
    "               random_state=42)\n",
    "\n",
    "if __name__=='__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-th",
   "language": "python",
   "name": "venv-th"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
