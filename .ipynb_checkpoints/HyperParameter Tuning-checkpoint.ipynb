{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\_vinc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\skopt\\space\\space.py:502: RuntimeWarning: divide by zero encountered in log10\n",
      "  np.log10(self.low) / self.log_base,\n",
      "c:\\users\\_vinc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\skopt\\space\\space.py:504: RuntimeWarning: divide by zero encountered in log10\n",
      "  np.log10(self.low) / self.log_base)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Eval num_timesteps=8000, episode_reward=-5287.40 +/- 235.41\n",
      "Episode length: 546.40 +/- 7.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-6016.20 +/- 1752.84\n",
      "Episode length: 591.40 +/- 27.78\n",
      "Eval num_timesteps=24000, episode_reward=-7448.20 +/- 2717.76\n",
      "Episode length: 781.00 +/- 49.49\n",
      "Ep:    84, steps: 1299, R: -12.000\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "from os.path import join, isfile\n",
    "from hyperspace import hyperdrive\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "import rl\n",
    "\n",
    "env_name = 'AbstractConveyor'\n",
    "\n",
    "\n",
    "path = pathlib.Path().absolute()\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function to be minimized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    * params [list, len(params)=n_hyperparameters]\n",
    "        Settings of each hyperparameter for a given optimization iteration.\n",
    "        - Controlled by hyperspaces's hyperdrive function.\n",
    "        - Order preserved from list passed to hyperdrive's hyperparameters argument.\n",
    "     \"\"\"\n",
    "    config_path = join(path, 'rl', 'config', '{}.yml'.format(env_name))\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        print('model loaded from path: {}'.format(config_path))\n",
    "    \n",
    "    #set the parameters\n",
    "    prfd, wsag, fr, nria, nrfeq, nrfc = params\n",
    "    config['environment']['positive_reward_for_divert'] = prfd\n",
    "    config['environment']['wrong_sup_at_goal'] = wsag\n",
    "    config['environment']['flooding_reward'] = fr\n",
    "    config['environment']['neg_reward_ia'] = nria\n",
    "    config['environment']['negative_reward_for_empty_queue'] = nrfeq\n",
    "    config['environment']['negative_reward_for_cycle'] = nrfc\n",
    "    \n",
    "    #GET MODEL CONFIG\n",
    "    model_config = config['models']['PPO2']\n",
    "    policy = config['main']['policy']\n",
    "    \n",
    "    # load environment with config variables\n",
    "    env_obj = getattr(rl.environments, env_name)\n",
    "    env = env_obj(config)\n",
    "    \n",
    "    # multiprocess environment\n",
    "    env_8 = make_vec_env(lambda: env, n_envs=8)\n",
    "    \n",
    "    #define folder and path\n",
    "    now = datetime.datetime.now()\n",
    "    folder ='{}{}{}_{}{}'.format(now.year, str(now.month).zfill(2), str(now.day).zfill(2), str(now.hour).zfill(2), str(now.minute).zfill(2))\n",
    "    specified_path = join(path, 'rl', 'trained_models', env_name, folder)\n",
    "    \n",
    "    # callback for evaluation\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=specified_path,\n",
    "                                 log_path=specified_path, eval_freq=1000,\n",
    "                                 n_eval_episodes=5, verbose=1,\n",
    "                                 deterministic=False, render=False)\n",
    "\n",
    "    model = PPO2(policy, env=env_8, tensorboard_log=specified_path, **model_config)\n",
    "    \n",
    "    #LEARN MODEL\n",
    "    model.learn(total_timesteps=80000, tb_log_name='{}_{}_{}_{}_{}_{}'.format(prfd, wsag, fr, nria, nrfeq, nrfc),\n",
    "                        callback=eval_callback)\n",
    "    model_path = join(specified_path, 'model_{}_{}_{}_{}_{}_{}.zip'.format(prfd, wsag, fr, nria, nrfeq, nrfc))\n",
    "    model.save(model_path)\n",
    "    \n",
    "    #test\n",
    "    best_modelpath = join(specified_path, 'best_model.zip')\n",
    "    test_model = PPO2.load(best_modelpath, env=DummyVecEnv([lambda: env]))\n",
    "    \n",
    "    #run test of the model\n",
    "    episodes = 10\n",
    "    results = {}\n",
    "    results['cycle_count'] = 0\n",
    "    results['idle_time'] = 0\n",
    "    for episode in range(episodes):\n",
    "        # Run an episode\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        meta_data = []\n",
    "        while not done:\n",
    "            action, _ = test_model.predict(state, deterministic=False)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                results['cycle_count'] += env.cyle_count\n",
    "                results['idle_time'] += sum(env.idle_times_operator.values())\n",
    "    \n",
    "    return (results['cycle_count'] + results['idle_time']) /episodes\n",
    "    \n",
    "def main():\n",
    "    #hparams = [(low, high),        #per var\n",
    "    #           (low, high)]\n",
    "    hparams = [(0, 10), #positive_reward_for_divert\n",
    "               (0, 10), #wrong_sup_at_goal\n",
    "               (0, 10), #flooding_reward\n",
    "               (0, 10), #neg_reward_ia\n",
    "               (0, 10), #negative_reward_for_empty_queue\n",
    "               (0, 10)] #negative_reward_for_cycle\n",
    "    \n",
    "    #define path for the results\n",
    "    hyperdive_results = join(path, 'rl', 'hyper_parameter')\n",
    "    \n",
    "    #make folder if not exist\n",
    "    try:\n",
    "        os.mkdir(hyperdive_results)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #run the hyper drive optimization\n",
    "    hyperdrive(objective=objective,\n",
    "               hyperparameters=hparams,\n",
    "               results_path=hyperdive_results,\n",
    "               checkpoints_path=hyperdive_results,\n",
    "               model=\"GP\",\n",
    "               n_iterations=50,\n",
    "               verbose=True,\n",
    "               random_state=42)\n",
    "\n",
    "if __name__=='__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-th",
   "language": "python",
   "name": "venv-th"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
